# -*- coding: utf-8 -*-
"""!!MRC detection real version3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KB28L0meuFrUWAEpeJn9KS20NLVAr6zz
"""



from google.colab import drive
drive.mount("/content/gdrive", force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/Parkteria/cococo.git
# %cd cococo

import os, json, cv2, numpy as np, matplotlib.pyplot as plt
from scipy.interpolate import UnivariateSpline
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
#from torch.utils.tensorboard import SummaryWriter

import torchvision
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.detection.keypoint_rcnn import KeypointRCNNPredictor
from torchvision.io import read_image
from torchvision import datapoints as dp
from torchvision.transforms.v2 import functional as F
from torchvision.transforms import v2 as T
from PIL import Image
import albumentations as A # Library for augmentations
import random
from sklearn.metrics import precision_recall_curve

# https://github.com/pytorch/vision/tree/main/references/detection
import utils
from utils import collate_fn
from engine import train_one_epoch, evaluate

def train_transform():
    return A.Compose([
        A.Sequential([
            A.OneOf([
              A.Rotate(limit=(0, 0), p=0.5), # 50% chance to rotate by 0 degrees
              A.Rotate(limit=(180, 180), p=0.5), # 50% chance to rotate by 180 degrees
        ], p=1),
            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1), # Random change of brightness & contrast
        ], p=1)
    ],
    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/
    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more at https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/
    )

class PennFudanDataset(torch.utils.data.Dataset):
    def __init__(self, root, transform=None, demo=False):
        self.root = root
        self.transform = transform
        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)
        self.imgs_files = sorted(os.listdir(os.path.join(root, "images")))
        self.annotations_files = sorted(os.listdir(os.path.join(root, "annotations")))

    def __getitem__(self, idx):
        img_path = os.path.join(self.root, "images", self.imgs_files[idx])
        annotations_path = os.path.join(self.root, "annotations", self.annotations_files[idx])

        img_original = cv2.imread(img_path)
        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)

        with open(annotations_path) as f:
            data = json.load(f)
            bboxes_original = data['bboxes']
            keypoints_original = data['keypoints']


            bboxes_labels_original = ['Tooth' for _ in bboxes_original]

        if self.transform:
            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints
            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):
            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format
            # Then we need to convert it to the following list:
            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]
            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]

            # Apply augmentations
            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)
            img = transformed['image']
            bboxes = transformed['bboxes']

            # Unflattening list transformed['keypoints']
            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):
            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format
            # Then we need to convert it to the following list:
            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]
            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,7,2)).tolist()

            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints
            keypoints = []
            for o_idx, obj in enumerate(keypoints_transformed_unflattened): # Iterating over objects
                obj_keypoints = []
                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object
                    # kp - coordinates of keypoint
                    # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint
                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])
                keypoints.append(obj_keypoints)

        else:
            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original

        # Convert everything into a torch tensor
        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)
        target = {}
        target["boxes"] = bboxes
        target["labels"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64) # all objects are glue tubes
        target["image_id"] = idx
        target["area"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])
        target["iscrowd"] = torch.zeros(len(bboxes), dtype=torch.int64)
        target["keypoints"] = torch.as_tensor(keypoints, dtype=torch.float32)
        img = F.to_tensor(img)

        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)
        target_original = {}
        target_original["boxes"] = bboxes_original
        target_original["labels"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are glue tubes
        target_original["image_id"] = idx
        target_original["area"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])
        target_original["iscrowd"] = torch.zeros(len(bboxes_original), dtype=torch.int64)
        target_original["keypoints"] = torch.as_tensor(keypoints_original, dtype=torch.float32)
        img_original = F.to_tensor(img_original)

        if self.demo:
            return img, target, img_original, target_original
        else:
            return img, target

    def __len__(self):
        return len(self.imgs_files)

KEYPOINTS_FOLDER_TRAIN = '/content/gdrive/My Drive/what/MRC/train'
dataset = PennFudanDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=True)
data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)

iterator = iter(data_loader)
batch = next(iterator)

print("Original targets:\n", batch[3], "\n\n")
print("Transformed targets:\n", batch[1])

keypoints_classes_ids2names = {0: 'CEJ(D)',1: 'CEJ(M)' ,2: 'Distal contact', 3: 'Distal papilla', 4: 'Mesial contact', 5: 'mesial papilla', 6: 'Recession'}

def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None):
    fontsize = 4

    for bbox in bboxes:
        start_point = (bbox[0], bbox[1])
        end_point = (bbox[2], bbox[3])
        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 1)

    for kps in keypoints:
        for idx, kp in enumerate(kps):
            image = cv2.circle(image.copy(), tuple(kp), 3, (255,0,0), 3)
            image = cv2.putText(image.copy(), " " + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 1, cv2.LINE_AA)

    if image_original is None and keypoints_original is None:
        plt.figure(figsize=(256,256))
        plt.imshow(image)

    else:
        for bbox in bboxes_original:
            start_point = (bbox[0], bbox[1])
            end_point = (bbox[2], bbox[3])
            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (0,255,0), 2)

        for kps in keypoints_original:
            for idx, kp in enumerate(kps):
                image_original = cv2.circle(image_original, tuple(kp), 3, (255,0,0), 3)
                image_original = cv2.putText(image_original, " " + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 1, cv2.LINE_AA)

        f, ax = plt.subplots(1, 2, figsize=(256, 256))

        ax[0].imshow(image_original)
        ax[0].set_title('Original image', fontsize=fontsize)

        ax[1].imshow(image)
        ax[1].set_title('Transformed image', fontsize=fontsize)

image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)
bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()

keypoints = []
for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():
    keypoints.append([kp[:2] for kp in kps])

image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)
bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()

keypoints_original = []
for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():
    keypoints_original.append([kp[:2] for kp in kps])

visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original)

def get_model(num_keypoints, weights_path=None):

    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512,), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))
    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,
                                                                   pretrained_backbone=True,
                                                                   num_keypoints = 7,
                                                                   num_classes = 2, # Background is the first class, object is the second class
                                                                   rpn_anchor_generator=anchor_generator)

    if weights_path:
        state_dict = torch.load(weights_path)
        model.load_state_dict(state_dict)

    return model

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

KEYPOINTS_FOLDER_TRAIN = '/content/gdrive/My Drive/what/MRC/train'
KEYPOINTS_FOLDER_TEST = '/content/gdrive/My Drive/what/MRC/test'

dataset_train = PennFudanDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)
dataset_test = PennFudanDataset(KEYPOINTS_FOLDER_TEST, transform= None, demo=False)

data_loader_train = DataLoader(dataset_train, batch_size=2, shuffle=True, collate_fn=collate_fn)
data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)

model = get_model(num_keypoints = 7)
model.to(device)

params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=0.0005)
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.3)
num_epochs = 10

for epoch in range(num_epochs):
    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=30)
    lr_scheduler.step()
    evaluate(model, data_loader_test, device)

torch.save(model.state_dict(), '/content/gdrive/My Drive/what/keypointsrcnn_weights.pth')

iterator = iter(data_loader_test)
images, targets = next(iterator)
images = list(image.to(device) for image in images)

with torch.no_grad():
    model.to(device)
    model.eval()
    output = model(images)

print("Predictions: \n", output)

all_images = []      # 전체 이미지를 저장할 리스트
all_keypoints = []   # 전체 키포인트를 저장할 리스트
all_bboxes = []

def predict_and_visualize(image_path):
    image = Image.open(image_path).convert("RGB")

    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)

    with torch.no_grad():
        model.to(device)
        model.eval()
        output = model(image_tensor)

    image_np = np.array(image)
    scores = output[0]['scores'].detach().cpu().numpy()

    high_scores_idxs = np.where(scores > 0.7)[0].tolist()
    post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy()

    keypoints = []
    for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():
        keypoints.append([list(map(int, kp[:2])) for kp in kps])

    bboxes = []
    for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():
        bboxes.append(list(map(int, bbox.tolist())))

    all_images.append(image_np)
    all_keypoints.extend(keypoints)
    all_bboxes.extend(bboxes)

    #visualize(image_np, bboxes, keypoints)

image_path = "/content/gdrive/My Drive/what/그림1.jpg"  # 원하는 이미지 경로로 변경
predict_and_visualize(image_path)

import matplotlib.patches as patches
plt.imshow(all_images[0])

keypoints_for_first_image = all_keypoints[0]
bboxes_for_first_image = all_bboxes[0]

for kp in keypoints_for_first_image:
    x, y = kp
    plt.scatter(x, y, c='r', marker='o')

x_min, y_min, x_max, y_max = bboxes_for_first_image
rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='g', facecolor='none')
plt.gca().add_patch(rect)

plt.show()

keypoints_for_first_image = all_keypoints[0]
for kp in keypoints_for_first_image:
    x, y = kp
    plt.scatter(x, y, c='r', marker='o')

def adjust_keypoints(keypoints):

    if len(keypoints) != 7:
        raise ValueError("Expected 7 keypoints, but got {}".format(len(keypoints)))
#하악의 경우 += 상악의 경우 -=
    keypoints[0][1] -= abs(keypoints[3][1] - keypoints[2][1])

    keypoints[1][1] -= abs(keypoints[4][1] - keypoints[5][1])

    return keypoints

def bezier_curve(point1, point2, control):
    t = np.linspace(0, 1, 1000)
    curve = np.outer((1 - t)**2, point1) + np.outer(2 * (1 - t) * t, control) + np.outer(t**2, point2)
    return curve[:, 0], curve[:, 1]
#상악의 경우 - 하악의 경우 +
def plot_curve_between_points(point1, point2):
    control_point = [(point1[0] + point2[0]) / 2, point1[1] - 5 * abs(point2[1] - point1[1])]

    xs, ys = bezier_curve(point1, point2, control_point)

    plt.plot(xs, ys, 'b-')

adjusted_keypoints = adjust_keypoints(all_keypoints[0])
print(adjusted_keypoints)

plt.imshow(all_images[0])

adjusted_keypoints = all_keypoints[0]

plt.scatter([kp[0] for kp in adjusted_keypoints], [kp[1] for kp in adjusted_keypoints], c='r', marker='o')


x_min, y_min, x_max, y_max = bboxes_for_first_image
rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='g', facecolor='none')
plt.gca().add_patch(rect)

plot_curve_between_points(adjusted_keypoints[0], adjusted_keypoints[1])

plt.show()

